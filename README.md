# MLLMsOnUMLCDs

> **Performance of Multimodal Large Language Models in Understanding Digital UML Class Diagrams**

This repository contains the datasets, evaluation scripts, prompts, and supplementary materials used in the study investigating how well **Multimodal Large Language Models (MLLMs)** can interpret and reason over **digital UML Class Diagrams (UMLCDs)**.

## ğŸ“Œ Overview

UML Class Diagrams are a cornerstone of software design, encoding structural relationships, attributes, methods, and constraints. As MLLMs (e.g., GPT-4V, LLaVA, Qwen-VL, Gemini) increasingly handle visual inputs, assessing their ability to "read" technical diagrams like UMLCDs becomes criticalâ€”for applications in automated documentation, design validation, and AI-assisted software engineering.

This project provides:

- A curated dataset of digital UML class diagrams (standardized formats, varied complexity)
- A benchmark suite with question-answer pairs targeting structural, behavioral, and semantic understanding
- Evaluation methodology and metrics (accuracy, robustness, error taxonomy)
- Prompt templates and few-shot examples used in experiments
- Analysis scripts and visualization tools

## ğŸ“‚ Repository Structure

â”œâ”€â”€ dataset/
â”‚ â”œâ”€â”€ UML class diagrams (PNG)
| â”œâ”€â”€ groundtruth (json)
â”œâ”€â”€ prompt/
â”‚ â””â”€â”€ Prompt.txt prompt template
â”œâ”€â”€ programs/
â”‚ â”œâ”€â”€ mainv2.0.py # a program to calculate metrics with a given prediction and groundtruth
â”‚ â””â”€â”€ run_evaluation.bat # a batch-processing program to run mainv2.0.py
â”œâ”€â”€ resutls/
â”‚ â”œâ”€â”€ average and standard deviation.csv # Taxonomy of common failure modes
â”‚ â””â”€â”€ metrics.csv # the resutls of precision, recall, and F1 score

## ğŸ› ï¸ Getting Started

### Prerequisites

- Python â‰¥ 3.9
- `pip install -r requirements.txt` (add this file later; recommend including: `pandas`, `matplotlib`, `torch`, `transformers`, `Pillow`, `openai`, etc.)

### Example: Running Evaluation

```bash
cd evaluation
python runner.py --model gpt-4o --data ../data/diagrams/ --questions ../data/questions.json --output results/gpt4o_results.json

@article{chai2025mllms,
  author    = {Chai, Wei and [Co-authors]},
  title     = {Performance of Multimodal Large Language Models in Understanding Digital UML Class Diagrams},
  journal   = {Submitted to [e.g., IEEE TSE / EMSE]},
  year      = {2025},
  note      = {Preprint: \url{https://arxiv.org/...}}
}
```
